<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>DU12w - DUW</title>
  <link rel="alternate" hreflang="en" href="http://du.zoomquiet.io/playground/" />

<meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="DebugUself" />
  <meta name="description" content="怼周刊_v12 ~ 预定 17.7.1 20:20 发布 时差 天圆地方自古曰 其实正好反过来 夜晩却为地自影 时区穿梭有差异 身体老实跟不上 总算归来查log 又是七天暗进步 迎新筹备" />

  <meta name="keywords" content="DU, weekly, github" />






<meta name="generator" content="Hugo 0.37.1" />


<link rel="canonical" href="http://du.zoomquiet.io/playground/post/du12w/" />

<link rel="apple-touch-icon" sizes="180x180" href="/playground/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/playground/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/playground/favicon-16x16.png">
<link rel="icon" href="/playground/favicon.ico" />
<link rel="manifest" href="/playground/manifest.json">
<link rel="mask-icon" href="/playground/safari-pinned-tab.svg" color="#5bbad5">




<link href="/playground/dist/jane.min.css?v=2.7.0" rel="stylesheet">
<link href="/playground/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="DU12w" />
<meta property="og:description" content="怼周刊_v12 ~ 预定 17.7.1 20:20 发布 时差 天圆地方自古曰 其实正好反过来 夜晩却为地自影 时区穿梭有差异 身体老实跟不上 总算归来查log 又是七天暗进步 迎新筹备" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://du.zoomquiet.io/playground/post/du12w/" />



<meta property="article:published_time" content="2017-07-01T20:20:00&#43;08:00"/>

<meta property="article:modified_time" content="2017-07-01T20:20:00&#43;08:00"/>











<meta itemprop="name" content="DU12w">
<meta itemprop="description" content="怼周刊_v12 ~ 预定 17.7.1 20:20 发布 时差 天圆地方自古曰 其实正好反过来 夜晩却为地自影 时区穿梭有差异 身体老实跟不上 总算归来查log 又是七天暗进步 迎新筹备">


<meta itemprop="datePublished" content="2017-07-01T20:20:00&#43;08:00" />
<meta itemprop="dateModified" content="2017-07-01T20:20:00&#43;08:00" />
<meta itemprop="wordCount" content="4938">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="DU12w"/>
<meta name="twitter:description" content="怼周刊_v12 ~ 预定 17.7.1 20:20 发布 时差 天圆地方自古曰 其实正好反过来 夜晩却为地自影 时区穿梭有差异 身体老实跟不上 总算归来查log 又是七天暗进步 迎新筹备"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->


<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-116546929-1', 'auto');
ga('send', 'pageview');
</script>
<script async src='//www.google-analytics.com/analytics.js'></script>



</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/playground/" class="logo">DU Weekly</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/playground/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/playground/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/playground/about/">
        <li class="mobile-menu-item">About</li>
      </a><a href="/playground/index.xml">
        <li class="mobile-menu-item">RSS</li>
      </a>
  </ul>
</nav>

  <header id="header" class="header container">
    <div class="logo-wrapper">
  <a href="/playground/" class="logo">DU Weekly</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/playground/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/playground/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/playground/about/">About</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/playground/index.xml">RSS</a>
      </li>
  </ul>
</nav>
  </header>

  <div id="mobile-panel">
    <main id="main" class="main bg-llight">
      <div class="content-wrapper">
        <div id="content" class="content container">
          <article class="post bg-white">
    
    <header class="post-header">
      <h1 class="post-title">DU12w</h1>

      <div class="post-meta">
        <span class="post-time"> 1077-07-07 </span>
        
        <span class="more-meta"> 4938 words </span>
        <span class="more-meta"> 10 min read </span>
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Table of Contents</h2>
  
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
<ul>
<li><a href="#怼周刊-v12">怼周刊_v12</a></li>
<li><a href="#进度">进度</a></li>
<li><a href="#任务">任务</a></li>
<li><a href="#进展">进展</a></li>
<li><a href="#成果">成果</a>
<ul>
<li><a href="#hetao-deep-learning学习笔记">@hetao - Deep Learning学习笔记</a>
<ul>
<li><a href="#20170626-skip-gram-word2-笔记">20170626 Skip-gram word2 笔记</a></li>
<li><a href="#第一部分-模型">第一部分:模型</a>
<ul>
<li><a href="#fake-task">Fake Task</a></li>
<li><a href="#模型细节">模型细节</a></li>
<li><a href="#隐层">隐层</a></li>
<li><a href="#输出层">输出层</a></li>
<li><a href="#直觉上的理解">直觉上的理解</a></li>
</ul></li>
<li><a href="#第二部分-基于skip-gram模型的高效训练">第二部分 基于skip-gram模型的高效训练</a>
<ul>
<li><a href="#对高频词抽样">对高频词抽样</a></li>
<li><a href="#抽样率">抽样率</a></li>
<li><a href="#负采样-negative-sampling">负采样(negative sampling)</a></li>
<li><a href="#如何选择negative-words">如何选择negative words</a></li>
</ul></li>
<li><a href="#第三部分-基于tensorflow实现skip-gram模型">第三部分:基于TensorFlow实现Skip-Gram模型</a>
<ul>
<li><a href="#参考资源">参考资源</a></li>
</ul></li>
<li><a href="#零碎卡片">零碎卡片</a>
<ul>
<li><a href="#word-embeddings">Word embeddings</a></li>
<li><a href="#word2vec">Word2Vec</a></li>
<li><a href="#preprocessing">Preprocessing</a></li>
<li><a href="#subsampling">Subsampling</a></li>
<li><a href="#making-batches">Making batches</a></li>
</ul></li>
</ul></li>
<li><a href="#zoejane-demo-网页">zoejane - Demo 网页</a></li>
<li><a href="#时间帐单-效能分析小队-推进中">时间帐单:效能分析小队 推进中</a></li>
</ul></li>
<li><a href="#故事">故事</a></li>
<li><a href="#推荐">推荐</a></li>
<li><a href="#后记">后记</a></li>
</ul>
</nav>
  </div>
</div>

    
    <div class="post-content">
      

<h1 id="怼周刊-v12">怼周刊_v12</h1>

<p>~ 预定 17.7.1 20:20 发布</p>

<hr />

<p>时差</p>

<pre><code>天圆地方自古曰
其实正好反过来
夜晩却为地自影
时区穿梭有差异
身体老实跟不上
总算归来查log
又是七天暗进步
迎新筹备还缺嘛
再启新四周项目
自怼小圈稳心气
</code></pre>

<hr />

<ul>
<li>主编: <a href="http://du.zoomquiet.io/2014-02/ac0-zq/">大妈</a></li>
<li>责编:

<ul>
<li><a href="http://du.zoomquiet.io/2017-04/about-xpgeng/">xpgeng</a></li>
<li><a href="http://du.zoomquiet.io/2017-04/about-sunoonlee/">sunoonlee</a></li>
<li><a href="http://du.zoomquiet.io/2017-04/about-zoe/">Zoe</a></li>
<li><a href="http://du.zoomquiet.io/2017-04/about-bambooom/">bambooom</a></li>
</ul></li>
</ul>

<h1 id="进度">进度</h1>

<p>~ 记录当周关键事件日期+证据链接</p>

<ul>
<li>170624 [72h [ANN] 170624 怼周会及会议纪要]<a href="https://github.com/DebugUself/du4proto/issues/148">6</a></li>
<li>170401 关闭报表和入密</li>
<li>170331 om103py 毕业</li>
</ul>

<h1 id="任务">任务</h1>

<p>~ 记述关键共怼任务 (如果没有, 留空)</p>

<ul>
<li>170624 <a href="https://github.com/DebugUself/du4proto/blob/master/S04E51/README.md">S04E51 启动</a></li>
<li>170603 <a href="https://github.com/DebugUself/du4proto/issues/135">怼圈的二次开放 筹备中</a></li>
</ul>

<h1 id="进展">进展</h1>

<p>~ 整体上圈内部活跃指标情况</p>

<ul>
<li>提交(S04E051): 7 人 (6人延续上期 + 1 个新项目)

<ul>
<li>小组 @zoomquiet 时间帐单:效能分析小队

<ul>
<li>@zoomquiet</li>
<li>@zsy</li>
<li>@liguanghe</li>
<li>@simpleowen</li>
<li>@mxclover</li>
</ul></li>
<li>@hetao Deep Learning 自学计划</li>
<li>@zoejane 进入 Web 世界</li>
</ul></li>
<li>引发的作品:

<ul>
<li>@hetao - Deep Learning 学习笔记</li>
<li>@zoejane - Demo 网页</li>
</ul></li>
<li>状态:</li>
</ul>

<table>
<tr><th>allcic Commit</th><th> times</th><th>weekly Commit</th><th> times</th></tr>
<tr><td>
            <a href='http://github.com/ZoomQuiet'>ZoomQuiet</a></td><td>255</td>
        <td>
            <a href='http://github.com/zoejane'>zoejane</a></td><td>25</td>
            
<tr><td>
            <a href='http://github.com/zoejane'>zoejane</a></td><td>239</td>
        <td>
            <a href='http://github.com/mxclover'>mxclover</a></td><td>10</td>
            
<tr><td>
            <a href='http://github.com/liguanghe'>liguanghe</a></td><td>117</td>
        <td>
            <a href='http://github.com/zhangshiyinrunwithcc'>zhangshiyinrunwithcc</a></td><td>2</td>
            
<tr><td>
            <a href='http://github.com/mxclover'>mxclover</a></td><td>113</td>
        <td>
            <a href='http://github.com/ZoomQuiet'>ZoomQuiet</a></td><td>1</td>
            
<tr><td>
            <a href='http://github.com/bambooom'>bambooom</a></td><td>107</td>
        <td>
            <a href='http://github.com/livingworld'>livingworld</a></td><td>1</td>
            
<tr><th>all Commit </th><th>Comments times</th><th>weekly Commit</th><th>Comments times</th></tr>
<tr><th>all Issue </th><th>Comments times</th><th>weekly Issue</th><th>Comments times</th></tr>
<tr><td>
            <a href='http://github.com/ZoomQuiet'>ZoomQuiet</a></td><td>307</td>
        <td>
            <a href='http://github.com/zhangshiyinrunwithcc'>zhangshiyinrunwithcc</a></td><td>8</td>
            
<tr><td>
            <a href='http://github.com/zhangshiyinrunwithcc'>zhangshiyinrunwithcc</a></td><td>168</td>
        <td>
            <a href='http://github.com/zoejane'>zoejane</a></td><td>3</td>
            
<tr><td>
            <a href='http://github.com/liguanghe'>liguanghe</a></td><td>129</td>
        <td>
            <a href='http://github.com/mxclover'>mxclover</a></td><td>1</td>
            
<tr><td>
            <a href='http://github.com/zoejane'>zoejane</a></td><td>70</td>
        <td>
            <a href='http://github.com/livingworld'>livingworld</a></td><td>1</td>
            
</table>

<p>&lt;- 170701 16:22</p>

<ul>
<li>在线(测试ing..):

<ul>
<li><code>curl du.zoomquiet.us</code></li>
<li><code>curl du.zoomquiet.us/v0/all/cic/rank/5/</code></li>
<li><code>curl du.zoomquiet.us/v0/all/cil/rank/5/</code></li>
<li><code>curl du.zoomquiet.us/v0/week/cic/rank/5/</code></li>
<li><code>curl du.zoomquiet.us/v0/week/cil/rank/5/</code></li>
</ul></li>
</ul>

<h1 id="成果">成果</h1>

<p>~ 各种成品/半成品 内部知识作品</p>

<h2 id="hetao-deep-learning学习笔记">@hetao - Deep Learning学习笔记</h2>

<ul>
<li><a href="https://github.com/DebugUself/du4proto/tree/hetao">DebugUself/du4proto at hetao</a></li>
<li>@hetao 童鞋近期在 GitHub 更新了自己这几周以来对于 Depp Learning 非常详细的学习笔记,推荐!</li>
</ul>

<h3 id="20170626-skip-gram-word2-笔记">20170626 Skip-gram word2 笔记</h3>

<p>Skip-Gram主要是给定input word来预测上下文,区别于CBOW给定上下文预测input word.</p>

<p>Word2Vec模型实际上分为了两个部分,第一部分为建立模型,第二部分是通过模型获取嵌入词向量.</p>

<h3 id="第一部分-模型">第一部分:模型</h3>

<h4 id="fake-task">Fake Task</h4>

<ul>
<li>印象:训练模型的真正目的是获得模型基于训练数据学得的隐层权重. 为了得到这些权重,我们首先需要构建完整的神经网络作为我们的&rdquo;Fake Task&rdquo;.</li>

<li><p>例子:以一个句子<code>&quot;The dog barked at the mailman&quot;</code>的输入为例.</p>

<ul>
<li>首选选择,单词&rdquo;dog&rdquo;作为input word.</li>
<li>其次,定义一个叫做<strong>skip_window</strong>的参数,它代表着我们从当前input word的一侧(左边或右边)选取词的数量. 如果我们设置<code>skip_window=2</code>,那么我们最终获得窗口中的词(包括input word在内)就是[&lsquo;The&rsquo;, &lsquo;dog&rsquo;,&lsquo;barked&rsquo;, &lsquo;at&rsquo;]. <code>skip_window=2</code>代表着选取左input word左侧2个词和右侧2个词进入我们的窗口,所以整个窗口大小span=2x2=4.</li>
<li>另一个参数叫<strong>num_skips</strong>,它代表着我们从整个窗口中选取多少个不同的词作为我们的output word,当<code>skip_window=2</code>,<code>num_skips=2</code>时,我们将会得到两组 (input word, output word) 形式的训练数据,即 (&lsquo;dog&rsquo;, &lsquo;barked&rsquo;),(&lsquo;dog&rsquo;, &lsquo;the&rsquo;).</li>
<li>神经网络基于这些训练数据将会输出一个概率分布,这个概率代表着我们的词典中的每个词是output word的可能性. 例如我们先拿一组数据 (&lsquo;dog&rsquo;, &lsquo;barked&rsquo;) 来训练神经网络,那么模型通过学习这个训练样本,会告诉我们词汇表中每个单词是&rdquo;barked&rdquo;的概率大小.</li>
<li>模型的输出概率代表着到我们词典中每个词有多大可能性跟input word同时出现.</li>

<li><p><strong>window_size=2</strong>表示选择输入词前后各两个词和输入词进行组合,如图蓝色代表input word,方框内代表位于窗口内的单词,由图可知,可以统计词汇组合出现概率高低.</p>

<ul>
<li><p><img src="https://raw.githubusercontent.com/DebugUself/du4proto/hetao/week8/note/images/window_size.png?token=ABQhvBWcLwFRicNJ1heHDKnP-yo3GPRRks5ZYEqlwA==" alt="" /></p>

<h4 id="模型细节">模型细节</h4></li>
</ul></li>
</ul></li>

<li><p>首选构建自己的词汇表(vocabulary)再对单词进行one-hot编码.</p></li>

<li><p>假设从我们的训练文档中抽取出10000个唯一不重复的单词组成词汇表. 我们对这10000个单词进行one-hot编码,得到的每个单词都是一个10000维的向量,向量每个维度的值只有0或者1,假如单词ants在词汇表中的出现位置为第3个,那么ants的向量就是一个第三维度取值为1,其他维都为0的10000维的向量($ants=[0, 0, 1, 0, &hellip;, 0]$).</p></li>

<li><p>还是上面的例子,&ldquo;The dog barked at the mailman&rdquo;,那么我们基于这个句子,可以构建一个大小为5的词汇表(忽略大小写和标点符号):(&ldquo;the&rdquo;, &ldquo;dog&rdquo;, &ldquo;barked&rdquo;, &ldquo;at&rdquo;, &ldquo;mailman&rdquo;),我们对这个词汇表的单词进行编号0-4. 那么&rdquo;dog&rdquo;就可以被表示为一个5维向量[0, 1, 0, 0, 0].</p></li>

<li><p>模型的输入如果为一个10000维的向量,那么输出也是一个10000维度(词汇表的大小)的向量,它包含了10000个概率,每一个概率代表着当前词是输入样本中output word的概率大小.</p></li>

<li><p><img src="https://raw.githubusercontent.com/DebugUself/du4proto/hetao/week8/note/images/neural_network.png?token=ABQhvNCC06XQfJwEMwBAZufGtntBrecgks5ZYEq5wA==" alt="" /></p></li>

<li><p>隐层没有使用任何激活函数,但是输出层使用了sotfmax.</p></li>

<li><p>我们基于成对的单词来对神经网络进行训练,训练样本是 ( input word, output word ) 这样的单词对,input word和output word都是one-hot编码的向量. 最终模型的输出是一个概率分布.</p></li>
</ul>

<h4 id="隐层">隐层</h4>

<p>用300个特征节点表示一个单词,即用300维向量表示,则隐藏层权重矩阵为10000x300维度.
看下面的图片,左右两张图分别从不同角度代表了输入层-隐层的权重矩阵. 左图中每一列代表一个10000维的词向量和隐层单个神经元连接的权重向量. 从右边的图来看,每一行实际上代表了每个单词的词向量.</p>

<p>上面我们提到,input word和output word都会被我们进行one-hot编码. 如果我们将一个1 x 10000的向量和10000 x 300的矩阵相乘,它会消耗相当大的计算资源,为了高效计算,它仅仅会选择矩阵中对应的向量中维度值为1的索引行(这句话很绕),看图就明白.</p>

<p>上面的例子中,左边向量中取值为1的对应维度为3(下标从0开始),那么计算结果就是矩阵的第3行(下标从0开始)&mdash; [10, 12, 19],这样模型中的隐层权重矩阵便成了一个&rdquo;查找表&rdquo;(lookup table),</p>

<h4 id="输出层">输出层</h4>

<p>经过神经网络隐层的计算,ants这个词会从一个1 x 10000的向量变成1 x 300的向量,再被输入到输出层. 输出层是一个softmax回归分类器,它的每个结点将会输出一个0-1之间的值(概率),这些所有输出层神经元结点的概率之和为1.
下面是一个例子,训练样本为 (input word: &ldquo;ants&rdquo;, output word: &ldquo;car&rdquo;) 的计算示意图.
<img src="https://raw.githubusercontent.com/DebugUself/du4proto/hetao/week8/note/images/input_output.png?token=ABQhvPeeIv6dMuE67Lo4hj5LYmjB2MnWks5ZYErMwA==" alt="" /></p>

<h4 id="直觉上的理解">直觉上的理解</h4>

<p>针对上下文相似的情况,进行词干话(stemming),就是去除词缀得到词根的过程.</p>

<h3 id="第二部分-基于skip-gram模型的高效训练">第二部分 基于skip-gram模型的高效训练</h3>

<p>针对权重矩阵过大的问题,Word2Vec做出了如下优化:</p>

<ul>
<li>将常见的单词组合(word pairs)或者词组作为单个&rdquo;words&rdquo;来处理.</li>
<li>对高频次单词进行抽样来减少训练样本的个数.</li>
<li>对优化目标采用&rdquo;negative sampling&rdquo;方法,这样每个训练样本的训练只会更新一小部分的模型权重,从而降低计算负担.</li>
</ul>

<p>这是一个<a href="http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/">模型词汇表</a>,及<a href="https://github.com/chrisjmccormick/inspect_word2vec/tree/master/vocabulary">vocabulary</a>,相应的论文有<a href="https://arxiv.org/pdf/1310.4546.pdf">Distributed Representations of Words and Phrases
and their Compositionality</a>,<a href="https://code.google.com/archive/p/word2vec/">代码</a>.</p>

<h4 id="对高频词抽样">对高频词抽样</h4>

<p>如下图所示,对于&rdquo;the&rdquo;这种常用高频单词,既无意义,又的比重大. 对于我们在训练原始文本中遇到的每一个单词,它们都有一定概率被我们从文本中删掉,而这个被删除的概率与单词的频率有关.
<img src="https://raw.githubusercontent.com/DebugUself/du4proto/hetao/week8/note/images/window_size.png?token=ABQhvLEjfwZz2EuoVM_DN8ZqQCWUNtrZks5ZYErdwA==" alt="" /></p>

<h4 id="抽样率">抽样率</h4>

<p>在语料库中,该词出现概率越低,越有可能被保留. $P(w_i)=1-\sqrt{\frac{t}{f(w_i)}}$. 在代码中还有一个参数叫&rdquo;sample&rdquo;,这个参数代表一个阈值,默认值为0.001,这个值越低,越不容易保留,即$P(w_i)$越大.</p>

<h4 id="负采样-negative-sampling">负采样(negative sampling)</h4>

<ul>
<li>印象:用来提高训练速度并且改善所得到词向量的质量的一种方法. 不同于原本每个训练样本更新所有的权重,负采样每次让一个训练样本仅仅更新一小部分的权重,这样就会降低梯度下降过程中的计算量.</li>
<li>例子:

<ul>
<li>当我们用训练样本 ( input word: &ldquo;fox&rdquo;,output word: &ldquo;quick&rdquo;) 来训练我们的神经网络时,&ldquo;fox&rdquo;和&rdquo;quick&rdquo;都是经过one-hot编码的. 如果我们的vocabulary大小为10000时,在输出层,我们期望对应&rdquo;quick&rdquo;单词的那个神经元结点输出1,其余9999个都应该输出0. 在这里,这9999个我们期望输出为0的神经元结点所对应的单词我们称为&rdquo;negative&rdquo; word.</li>
<li>如果使用了负采样的方法我们仅仅去更新我们的positive word-&ldquo;quick&rdquo;的和我们选择的其他5个negative words的结点对应的权重,共计6个输出神经元,相当于每次只更新$300x 6=1800$个权重.</li>
<li>在论文中,作者指出指出对于小规模数据集,选择5-20个negative words会比较好,对于大规模数据集可以仅选择2-5个negative words.</li>
</ul></li>
</ul>

<h4 id="如何选择negative-words">如何选择negative words</h4>

<p>我们使用&rdquo;一元模型分布(unigram distribution)&ldquo;来选择&rdquo;negative words&rdquo;. 要注意的一点是,一个单词被选作negative sample的概率跟它出现的频次有关,出现频次越高的单词越容易被选作negative words.
代码公式如下:
$P(w_i)=\frac{f(w_i)^{<sup>3</sup>&frasl;<sub>4</sub>}}{\sum_{j=0}^n(f(w_j)^{<sup>3</sup>&frasl;<sub>4</sub>})}$
每个单词被赋予一个权重,即$f(w_i)$, 它代表着单词出现的频次. 公式中开3/4的根号完全是基于经验的,论文中提到这个公式的效果要比其它公式更加出色.</p>

<ul>
<li><a href="https://github.com/chrisjmccormick/word2vec_commented">c语言实现代码</a></li>
<li><a href="https://github.com/chrisjmccormick/word2vec_commented">Word2Vec Resources教程</a></li>
</ul>

<h3 id="第三部分-基于tensorflow实现skip-gram模型">第三部分:基于TensorFlow实现Skip-Gram模型</h3>

<p>分为四部分:</p>

<ul>
<li>数据预处理</li>

<li><p>训练样本构建</p>

<ul>
<li>采样</li>
<li>构造batch

<ul>
<li>找到每个input word的上下文:如果我们固定skip_window=2的话,那么fox的上下文就是[quick, brown, jumps, over].

<ul>
<li>我在实际选择input word上下文时,使用的窗口大小是一个介于[1, window_size]区间的随机数. 这里的目的是让模型更多地去关注离input word更近词.</li>
</ul></li>
<li>基于上下文构建batch:如果我们的batch_size=1的话,那么实际上一个batch中有四个训练样本.</li>
</ul></li>
</ul></li>

<li><p>模型构建</p>

<ul>
<li>输入层到嵌入层

<ul>
<li>输入层到隐层的权重矩阵作为嵌入层要给定其维度,一般embeding_size设置为50-300之间.</li>
</ul></li>
<li>嵌入层到输出层

<ul>
<li>TensorFlow中的sampled_softmax_loss,由于进行了negative sampling,所以实际上我们会低估模型的训练loss.

<ul>
<li>有点儿费解.</li>
</ul></li>
</ul></li>
</ul></li>

<li><p>模型验证</p></li>
</ul>

<h4 id="参考资源">参考资源</h4>

<ul>
<li><a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">A really good conceptual overview of word2vec from Chris McCormick</a></li>
<li><a href="https://arxiv.org/pdf/1301.3781.pdf">First word2vec paper from Mikolov et al.</a></li>
<li><a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">NIPS paper with improvements for word2vec also from Mikolov et al.</a></li>
<li><a href="http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/">An implementation of word2vec from Thushan Ganegedara</a></li>
<li><a href="https://www.tensorflow.org/tutorials/word2vec">TensorFlow word2vec tutorial</a></li>
<li><a href="https://zhuanlan.zhihu.com/zhaoyeyu">知乎专栏</a></li>
</ul>

<h3 id="零碎卡片">零碎卡片</h3>

<h4 id="word-embeddings">Word embeddings</h4>

<ul>
<li>印象:one-hot encode效率太低,浪费计算资源. 为了解决这个问题,采用了所谓embeddings,通过直接从权重矩阵中获取隐藏层值来跳过嵌入层的乘法,即使用权重矩阵作为查找表.</li>
<li>例子:例如&rdquo;heart&rdquo;被编码为958,&ldquo;mind&rdquo;为18094.然后为了获取&rdquo;heart&rdquo;的隐藏层值,只需要嵌入矩阵的第958行. 这个过程称为嵌入式查找,隐藏单元的数量是嵌入维度.

<ul>
<li><img src="https://raw.githubusercontent.com/DebugUself/du4proto/hetao/week8/note/images/lookup_matrix.png?token=ABQhvOms5WWP6LWu8qx0iukDk9CYQ4Ouks5ZYEtMwA==" alt="" /></li>
<li><img src="https://raw.githubusercontent.com/DebugUself/du4proto/hetao/week8/note/images/tokenize_lookup.png?token=ABQhvLHc3PWG3nLblV-ZTa1gk5jGDxpPks5ZYEtzwA==" alt="" /></li>
</ul></li>
</ul>

<h4 id="word2vec">Word2Vec</h4>

<ul>
<li>印象:两种实现Word2Vec的结构:

<ul>
<li>CBOW (Continuous Bag-Of-Words)</li>
<li>Skip-gram
<img src="https://raw.githubusercontent.com/DebugUself/du4proto/hetao/week8/note/images/word2vec_architectures.png?token=ABQhvNFrrnOwVj9uK9LSkao02U8ZSecoks5ZYEuKwA==" alt="" /></li>
</ul></li>
</ul>

<h4 id="preprocessing">Preprocessing</h4>

<p>印象:根据词频排序,频率最高的为0,其次为1,依次排序. 最终,输入一段words,输出的是一段文字中单词对应的序列位置.</p>

<h4 id="subsampling">Subsampling</h4>

<ul>
<li>印象:去除一些无意义的词汇. 公式表达式为:
$P(w_i)=1-\sqrt{\frac{t}{f(w_i)}}$
其中,t是惩罚参数,$f(w_i)$是$w_i$在数据集中的频率,$P(w_i)$是丢弃word的概率</li>
<li>例子:将一些无意义的词汇,例如&rdquo;the&rdquo;,&ldquo;of&rdquo;等去除.</li>
</ul>

<h4 id="making-batches">Making batches</h4>

<ul>
<li>印象:获取一段话前后的word个数.</li>
<li>例子:</li>
</ul>

<pre><code>def get_target(words, idx, window_size=5):
    ''' Get a list of words in a window around an index. '''
    R = np.random.randint(1, window_size+1)
    start = idx - R if (idx - R) &gt; 0 else 0
    stop = idx + R
    target_words = set(words[start:idx] + words[idx+1:stop+1]) # 切片之后打乱了顺序

    # Your code here
    return list(target_words)
</code></pre>

<h2 id="zoejane-demo-网页">zoejane - Demo 网页</h2>

<ul>
<li><a href="https://github.com/DebugUself/du4proto/blob/master/S04E51/du_s04e51_zoejane_plan.md">Project Plan</a></li>
<li><a href="http://blog.zoejane.net/learn-web">Demo 网页</a></li>
</ul>

<h2 id="时间帐单-效能分析小队-推进中">时间帐单:效能分析小队 推进中</h2>

<ul>
<li>组员

<ul>
<li><code>(￣▽￣)</code> -&gt; 大妈</li>
<li>🐻 <code>熊</code> =&gt; @zhangshiyinrunwithcc</li>
<li>🐣 <code>鹤</code> =&gt; @李广鹤</li>
<li>🐈 <code>猫</code> =&gt; @simpleowen</li>
<li>🐴 <code>mx</code> =&gt; @mxclover</li>
</ul></li>
<li>目标

<ul>
<li>通过分析大妈和剑飞, 两人5年以上持续时间帐单的数据</li>
<li>获得数据化的行为效能结论</li>
<li>对自身行为给出几点优化策略</li>
</ul></li>
<li><a href="https://github.com/DebugUself/du4proto/tree/atl4dama">Github 项目链接</a></li>
<li><a href="https://github.com/DebugUself/du4proto/blob/master/S03E51/du_s03e51_zoomquiet_plan.md">Project Plan</a><br /></li>
</ul>

<h1 id="故事">故事</h1>

<p>~ 收集各自无法雷同的怼圈真人故事&hellip;</p>

<h1 id="推荐">推荐</h1>

<p>~ 嗯哼各种怼路上发现的嗯哼&hellip;</p>

<h1 id="后记">后记</h1>

<p>~ 怼周刊是什么以及为什么和能怎么&hellip;</p>

<p>大妈曰过: <code>参差多态 才是生机</code>
问题在 <code>参差</code> 的行为是无法形成团队的</p>

<pre><code>Coming together is a beginning; 
Keeping together is progress; 
Working together is success!
</code></pre>

<p>&lt;&mdash; <a href="https://www.brainyquote.com/quotes/quotes/h/henryford121997.html">Henry Ford</a></p>

<ul>
<li>所以, 有了 大妈 随见随怼的持续嗯哼&hellip;</li>
<li>但是, 想象一年后, 回想几十周前自己作的那些 <code>图样图森破</code></li>
<li>却没现成的资料来出示给后进来嗯哼?</li>
<li>不科学, 值得记录的, 就应当有个形式固定下来</li>
<li>所以,有了这个 <code>怼周刊</code> (Weekly 4 DU)</li>
</ul>

    </div>

    
    
<div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">DebugUself</span>
  </p>
  
  <p class="copyright-item">
    <span class="item-title">License</span>
    <span class="item-content"><a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a></span>
  </p>
</div>

    
    

    <footer class="post-footer">
      

      
      <nav class="post-nav">
        
          <a class="prev" href="/playground/post/du13w/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">DU13w</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/playground/post/du11w/">
            <span class="next-text nav-default">DU11w</span>
            <span class="prev-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
    

  <div class="disqus-button" id="load_disqus" onclick="load_disqus()">
      Show Disqus Comments
    </div>
    <div id="disqus_thread"></div>
    <script type="text/javascript">
    function load_disqus() {
        
        
        if (window.location.hostname === 'localhost') return;

        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        var disqus_shortname = 'duw';
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);

        $('#load_disqus').remove();
    };
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    

  

  
  </article>
        </div>
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="https://github.com/DebugUself/" rel="me" class="iconfont icon-github" title="github"></a>
      <a href="mailto:du@zoejane.net" rel="me" class="iconfont icon-email" title="email"></a>
  <a href="http://du.zoomquiet.io/playground/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - <a class="theme-link" href="https://github.com/xianmin/hugo-theme-jane">Jane</a>
  </span>

  <span class="copyright-year">
    &copy; 
    
      2017 - 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">DebugUself</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/playground/lib/highlight/highlight.pack.js?v=20171001"></script>
<script type="text/javascript" src="/playground/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/playground/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/playground/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>
<script type="text/javascript" src="/playground/dist/jane.min.js?v=2.7.0"></script>





</body>
</html>
